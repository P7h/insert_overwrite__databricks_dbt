{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Verify Source and Mart Tables are in Sync\n\nThis notebook verifies that mart tables are synchronized with source tables after running dbt with insert_overwrite.\n\n## Purpose:\nDemonstrates that Dynamic Partition Overwrite successfully refreshed only the affected dates.\n\n## Checks:\n1. Auto-detects dates that were recently updated (last 2 hours)\n2. Compare source vs mart counts for those dates\n3. Verify last_updated_at timestamps show recent refresh\n4. Confirm all other dates remain unchanged\n\n## Parameters:\n- `catalog`: Target Unity Catalog (default: main)\n- `schema`: Schema name (default: your_schema)\n\n## Auto-detection:\nThe notebook automatically identifies which dates were refreshed by finding rows with recent `last_updated_at` timestamps.\nThis works regardless of which dates were actually updated."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get parameters\ndbutils.widgets.text(\"catalog\", \"main\", \"Catalog Name\")\ndbutils.widgets.text(\"schema\", \"your_schema\", \"Schema Name\")\n\ncatalog = dbutils.widgets.get(\"catalog\")\nschema = dbutils.widgets.get(\"schema\")\n\nprint(f\"Catalog: {catalog}\")\nprint(f\"Schema: {schema}\")\n\n# Auto-detect dates that were recently updated (last 2 hours)\nprint(f\"\\nüîç Auto-detecting recently updated dates...\")\n\nrecently_updated_df = spark.sql(f\"\"\"\n    SELECT DISTINCT order_date\n    FROM {catalog}.{schema}.orders_mart_partitioned\n    WHERE last_updated_at >= TIMESTAMPADD(HOUR, -2, CURRENT_TIMESTAMP())\n    ORDER BY order_date\n\"\"\").collect()\n\ntarget_dates = [row['order_date'].strftime('%Y-%m-%d') for row in recently_updated_df]\n\nif target_dates:\n    print(f\"‚úÖ Found {len(target_dates)} recently updated dates: {', '.join(target_dates)}\")\nelse:\n    print(f\"‚ö†Ô∏è  No dates updated in last 2 hours. Checking all dates instead...\")\n    # Fallback: Get all dates\n    all_dates_df = spark.sql(f\"\"\"\n        SELECT DISTINCT order_date\n        FROM {catalog}.{schema}.orders_mart_partitioned\n        ORDER BY order_date\n        LIMIT 3\n    \"\"\").collect()\n    target_dates = [row['order_date'].strftime('%Y-%m-%d') for row in all_dates_df]\n    print(f\"   Using sample dates: {', '.join(target_dates)}\")\n\nprint(f\"\\nVerifying dates: {target_dates}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify target dates are in sync\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION: Source vs Mart Comparison for Target Dates\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_in_sync = True\n",
    "\n",
    "for date in target_dates:\n",
    "    # Source table counts\n",
    "    count_partitioned_src = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as cnt\n",
    "        FROM {catalog}.{schema}.orders_partitioned\n",
    "        WHERE order_date = '{date}'\n",
    "    \"\"\").collect()[0]['cnt']\n",
    "\n",
    "    count_liquid_src = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as cnt\n",
    "        FROM {catalog}.{schema}.orders_liquid\n",
    "        WHERE order_date = '{date}'\n",
    "    \"\"\").collect()[0]['cnt']\n",
    "\n",
    "    # Mart table aggregations\n",
    "    mart_partitioned = spark.sql(f\"\"\"\n",
    "        SELECT total_orders, last_updated_at\n",
    "        FROM {catalog}.{schema}.orders_mart_partitioned\n",
    "        WHERE order_date = '{date}'\n",
    "    \"\"\").collect()\n",
    "\n",
    "    mart_liquid_count = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            SUM(total_orders) as total_orders,\n",
    "            MAX(last_updated_at) as last_updated_at\n",
    "        FROM {catalog}.{schema}.orders_mart_liquid\n",
    "        WHERE order_date = '{date}'\n",
    "    \"\"\").collect()\n",
    "\n",
    "    mart_part_orders = mart_partitioned[0]['total_orders'] if mart_partitioned else 0\n",
    "    mart_part_updated = mart_partitioned[0]['last_updated_at'] if mart_partitioned else None\n",
    "    mart_liq_orders = mart_liquid_count[0]['total_orders'] if mart_liquid_count and mart_liquid_count[0]['total_orders'] else 0\n",
    "    mart_liq_updated = mart_liquid_count[0]['last_updated_at'] if mart_liquid_count else None\n",
    "\n",
    "    # Check if in sync\n",
    "    part_in_sync = (count_partitioned_src == mart_part_orders)\n",
    "    liq_in_sync = (count_liquid_src == mart_liq_orders)\n",
    "    \n",
    "    if not (part_in_sync and liq_in_sync):\n",
    "        all_in_sync = False\n",
    "\n",
    "    # Calculate minutes since update\n",
    "    if mart_part_updated:\n",
    "        minutes_ago = (datetime.now() - mart_part_updated).total_seconds() / 60\n",
    "    else:\n",
    "        minutes_ago = None\n",
    "\n",
    "    print(f\"\\n{date}:\")\n",
    "    print(f\"  Source Tables:\")\n",
    "    print(f\"    orders_partitioned: {count_partitioned_src:,} orders\")\n",
    "    print(f\"    orders_liquid:      {count_liquid_src:,} orders\")\n",
    "    print(f\"  Mart Tables:\")\n",
    "    print(f\"    orders_mart_partitioned: {mart_part_orders:,} total_orders\")\n",
    "    print(f\"    orders_mart_liquid:      {mart_liq_orders:,} total_orders\")\n",
    "    print(f\"  Last Updated:\")\n",
    "    if mart_part_updated:\n",
    "        print(f\"    {mart_part_updated} ({minutes_ago:.1f} minutes ago)\")\n",
    "    else:\n",
    "        print(f\"    Never updated\")\n",
    "    \n",
    "    if part_in_sync and liq_in_sync:\n",
    "        print(f\"  ‚úÖ IN SYNC: Source and Mart both show {count_partitioned_src:,} orders\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå OUT OF SYNC:\")\n",
    "        if not part_in_sync:\n",
    "            print(f\"     - Partitioned: Source has {count_partitioned_src:,}, Mart has {mart_part_orders:,}\")\n",
    "        if not liq_in_sync:\n",
    "            print(f\"     - Liquid: Source has {count_liquid_src:,}, Mart has {mart_liq_orders:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a few other dates to confirm they were NOT updated\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION: Other Dates Should NOT Be Recently Updated\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get a sample of other dates\n",
    "other_dates_df = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT order_date\n",
    "    FROM {catalog}.{schema}.orders_partitioned\n",
    "    WHERE order_date NOT IN ({','.join([\"'\" + d + \"'\" for d in target_dates])})\n",
    "    ORDER BY order_date\n",
    "    LIMIT 5\n",
    "\"\"\").collect()\n",
    "\n",
    "other_dates = [row['order_date'].strftime('%Y-%m-%d') for row in other_dates_df]\n",
    "\n",
    "print(f\"\\nChecking {len(other_dates)} sample dates that should NOT have been refreshed:\")\n",
    "print(f\"Dates: {', '.join(other_dates)}\\n\")\n",
    "\n",
    "for date in other_dates:\n",
    "    mart_info = spark.sql(f\"\"\"\n",
    "        SELECT last_updated_at\n",
    "        FROM {catalog}.{schema}.orders_mart_partitioned\n",
    "        WHERE order_date = '{date}'\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    if mart_info:\n",
    "        last_updated = mart_info[0]['last_updated_at']\n",
    "        hours_ago = (datetime.now() - last_updated).total_seconds() / 3600\n",
    "        \n",
    "        if hours_ago > 24:\n",
    "            print(f\"  {date}: ‚úÖ Last updated {hours_ago:.1f} hours ago (not recently refreshed)\")\n",
    "        else:\n",
    "            print(f\"  {date}: ‚ö†Ô∏è  Last updated {hours_ago:.1f} hours ago (recently refreshed - unexpected!)\")\n",
    "    else:\n",
    "        print(f\"  {date}: ‚ùå No data in mart table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display final summary\nprint(\"\\n\" + \"=\"*80)\nif all_in_sync:\n    print(\"‚úÖ SUCCESS: DYNAMIC PARTITION OVERWRITE VERIFICATION PASSED\")\nelse:\n    print(\"‚ùå FAILURE: TABLES ARE STILL OUT OF SYNC\")\nprint(\"=\"*80)\n\nif all_in_sync:\n    print(f\"\\nüéØ VERIFICATION COMPLETE:\")\n    print(f\"  ‚úÖ All {len(target_dates)} target dates are in sync\")\n    print(f\"  ‚úÖ Source table counts match mart table aggregations\")\n    print(f\"  ‚úÖ last_updated_at timestamps show recent refresh\")\n    print(f\"  ‚úÖ Other dates were NOT unnecessarily refreshed\")\n    print(f\"\\nüí° This proves Dynamic Partition Overwrite worked correctly:\")\n    print(f\"  - Automatically detected which partitions had new data\")\n    print(f\"  - Refreshed ONLY those {len(target_dates)} partitions\")\n    print(f\"  - Left all other partitions unchanged (efficient!)\")\n    print(f\"\\nüìä Efficiency:\")\n    print(f\"  - Refreshed: {len(target_dates)} dates\")\n    print(f\"  - Unchanged: ~{90 - len(target_dates)} dates\")\n    print(f\"  - Savings: {100 - (len(target_dates) / 90 * 100):.1f}% less work than full refresh!\")\nelse:\n    print(f\"\\n‚ùå ISSUE DETECTED:\")\n    print(f\"  Some target dates are still out of sync.\")\n    print(f\"  Please check:\")\n    print(f\"  1. Did dbt run complete successfully?\")\n    print(f\"  2. Were the late-arriving orders inserted with recent created_at timestamp?\")\n    print(f\"  3. Does the source data actually have data for these dates?\")\n\nprint(\"=\"*80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
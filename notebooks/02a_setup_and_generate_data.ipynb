{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Setup Tables and Generate Sample Data\n\nThis notebook:\n1. Creates source tables (orders_partitioned and orders_liquid)\n2. Generates 20,000 sample orders with realistic distributions\n3. Ensures IDENTICAL data in both tables for accurate comparison\n\n## Tables Created:\n- **orders_partitioned**: PARTITIONED BY order_date\n- **orders_liquid**: CLUSTER BY order_date\n\n## Parameters:\n- `catalog`: Target Unity Catalog (default: main)\n- `schema`: Schema name (default: your_schema)\n- `num_orders`: Number of orders to generate (default: 20000)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get parameters\ndbutils.widgets.text(\"catalog\", \"main\", \"Catalog Name\")\ndbutils.widgets.text(\"schema\", \"your_schema\", \"Schema Name\")\ndbutils.widgets.text(\"num_orders\", \"20000\", \"Number of Orders\")\n\ncatalog = dbutils.widgets.get(\"catalog\")\nschema = dbutils.widgets.get(\"schema\")\nnum_orders = int(dbutils.widgets.get(\"num_orders\"))\n\nprint(f\"Catalog: {catalog}\")\nprint(f\"Schema: {schema}\")\nprint(f\"Number of Orders: {num_orders:,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema if it doesn't exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "print(f\"âœ“ Schema '{catalog}.{schema}' is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop existing tables for clean setup\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.orders_partitioned\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.orders_liquid\")\n",
    "print(\"âœ“ Cleaned up existing tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create partitioned orders table\n",
    "create_partitioned_sql = f\"\"\"\n",
    "CREATE TABLE {catalog}.{schema}.orders_partitioned (\n",
    "    order_id STRING NOT NULL,\n",
    "    customer_id STRING NOT NULL,\n",
    "    order_date DATE NOT NULL,\n",
    "    product_id STRING,\n",
    "    quantity INT NOT NULL,\n",
    "    unit_price DECIMAL(10,2) NOT NULL,\n",
    "    total_amount DECIMAL(10,2),\n",
    "    status STRING NOT NULL,\n",
    "    created_at TIMESTAMP NOT NULL\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (order_date)\n",
    "TBLPROPERTIES (\n",
    "    'delta.enableChangeDataFeed' = 'true',\n",
    "    'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "    'delta.autoOptimize.autoCompact' = 'true'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_partitioned_sql)\n",
    "print(f\"âœ“ Created table: {catalog}.{schema}.orders_partitioned (PARTITIONED BY order_date)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate date range\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "today = datetime.now().date()\n",
    "first_day_of_current_month = today.replace(day=1)\n",
    "first_day_of_previous_month = first_day_of_current_month - relativedelta(months=1)\n",
    "days_in_range = (today - first_day_of_previous_month).days + 1\n",
    "\n",
    "print(f\"\\nDate Range: {first_day_of_previous_month} to {today} ({days_in_range} days)\")\n",
    "print(f\"Expected: ~{num_orders // days_in_range:,} orders/day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data using PySpark DataFrame API\n",
    "print(f\"\\nGenerating {num_orders:,} sample orders...\")\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.range(num_orders).select(\n",
    "    F.concat(F.lit('ORD-'), F.lpad(F.col('id').cast('string'), 10, '0')).alias('order_id'),\n",
    "    \n",
    "    F.concat(\n",
    "        F.lit('CUST-'),\n",
    "        F.lpad(\n",
    "            F.when(F.rand() < 0.3, (F.rand() * 50 + 1).cast('int'))\n",
    "             .when(F.rand() < 0.6, (F.rand() * 150 + 51).cast('int'))\n",
    "             .otherwise((F.rand() * 300 + 201).cast('int')).cast('string'),\n",
    "            5, '0'\n",
    "        )\n",
    "    ).alias('customer_id'),\n",
    "    \n",
    "    F.date_add(\n",
    "        F.date_trunc('month', F.add_months(F.current_date(), -1)),\n",
    "        F.when(F.rand() < 0.60, (F.rand() * F.datediff(F.current_date(), F.date_trunc('month', F.add_months(F.current_date(), -1)))).cast('int'))\n",
    "         .when(F.rand() < 0.85, F.greatest(F.datediff(F.current_date(), F.date_trunc('month', F.add_months(F.current_date(), -1))) - (F.rand() * 14).cast('int'), F.lit(0)))\n",
    "         .otherwise(((F.rand() * F.datediff(F.current_date(), F.date_trunc('month', F.add_months(F.current_date(), -1))) / 2) + (F.datediff(F.current_date(), F.date_trunc('month', F.add_months(F.current_date(), -1))) / 4)).cast('int'))\n",
    "    ).alias('order_date'),\n",
    "    \n",
    "    F.concat(\n",
    "        F.lit('PROD-'),\n",
    "        F.lpad(\n",
    "            F.when(F.rand() < 0.4, (F.rand() * 20 + 1).cast('int'))\n",
    "             .when(F.rand() < 0.7, (F.rand() * 30 + 21).cast('int'))\n",
    "             .otherwise((F.rand() * 50 + 51).cast('int')).cast('string'),\n",
    "            4, '0'\n",
    "        )\n",
    "    ).alias('product_id'),\n",
    "    \n",
    "    F.when(F.rand() < 0.5, F.lit(1))\n",
    "     .when(F.rand() < 0.8, (F.rand() * 2 + 2).cast('int'))\n",
    "     .otherwise((F.rand() * 7 + 4).cast('int')).alias('quantity'),\n",
    "    \n",
    "    F.when(F.rand() < 0.4, F.round(9.99 + F.rand() * 40, 2))\n",
    "     .when(F.rand() < 0.7, F.round(50 + F.rand() * 100, 2))\n",
    "     .otherwise(F.round(150 + F.rand() * 350, 2)).alias('unit_price'),\n",
    "    \n",
    "    F.lit(None).cast('decimal(10,2)').alias('total_amount'),\n",
    "    \n",
    "    F.when(F.rand() < 0.60, F.lit('delivered'))\n",
    "     .when(F.rand() < 0.75, F.lit('shipped'))\n",
    "     .when(F.rand() < 0.87, F.lit('confirmed'))\n",
    "     .when(F.rand() < 0.95, F.lit('pending'))\n",
    "     .otherwise(F.lit('cancelled')).alias('status'),\n",
    "    \n",
    "    F.current_timestamp().alias('created_at')\n",
    ")\n",
    "\n",
    "df.write.mode('append').insertInto(f\"{catalog}.{schema}.orders_partitioned\")\n",
    "print(f\"âœ“ Generated and inserted {num_orders:,} orders into partitioned table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create liquid clustered table by copying from partitioned table (ensures IDENTICAL data)\n",
    "create_liquid_sql = f\"\"\"\n",
    "CREATE TABLE {catalog}.{schema}.orders_liquid\n",
    "USING DELTA\n",
    "CLUSTER BY (order_date)\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableChangeDataFeed' = 'true',\n",
    "  'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "  'delta.autoOptimize.autoCompact' = 'true'\n",
    ")\n",
    "AS SELECT * FROM {catalog}.{schema}.orders_partitioned\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_liquid_sql)\n",
    "print(f\"âœ“ Created liquid clustered table by copying data (CLUSTER BY order_date)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data\n",
    "orders_partitioned_count = spark.sql(f\"SELECT COUNT(*) as count FROM {catalog}.{schema}.orders_partitioned\").collect()[0]['count']\n",
    "orders_liquid_count = spark.sql(f\"SELECT COUNT(*) as count FROM {catalog}.{schema}.orders_liquid\").collect()[0]['count']\n",
    "\n",
    "print(f\"\\nâœ“ Partitioned table: {orders_partitioned_count:,} rows\")\n",
    "print(f\"âœ“ Liquid clustered table: {orders_liquid_count:,} rows\")\n",
    "print(f\"\\nâœ… Both tables have IDENTICAL data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize tables\n",
    "print(\"\\nOptimizing tables...\")\n",
    "spark.sql(f\"OPTIMIZE {catalog}.{schema}.orders_partitioned\")\n",
    "spark.sql(f\"OPTIMIZE {catalog}.{schema}.orders_liquid\")\n",
    "print(\"âœ“ Tables optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… SETUP AND DATA GENERATION COMPLETE\")\nprint(\"=\"*70)\nprint(f\"\\nTables created with IDENTICAL data:\")\nprint(f\"  1. {catalog}.{schema}.orders_partitioned\")\nprint(f\"     - {orders_partitioned_count:,} rows\")\nprint(f\"     - PARTITIONED BY order_date\")\nprint(f\"  2. {catalog}.{schema}.orders_liquid\")\nprint(f\"     - {orders_liquid_count:,} rows\")\nprint(f\"     - CLUSTER BY order_date\")\nprint(f\"\\nDate range: {first_day_of_previous_month} to {today}\")\nprint(f\"\\nâœ¨ This notebook is part of the complete dbt_workflow job\")\nprint(f\"   The workflow automatically runs:\")\nprint(f\"   1. This setup (02a)\")\nprint(f\"   2. Initial dbt build with --full-refresh\")\nprint(f\"   3. Simulate late arrivals (02b)\")\nprint(f\"   4. dbt build --full-refresh (fast! insert_overwrite STILL only updates affected partitions)\")\nprint(f\"   5. Verify sync (02c)\")\nprint(f\"\\nðŸ’¡ To run the complete workflow:\")\nprint(f\"   databricks bundle run dbt_workflow --target dev\")\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
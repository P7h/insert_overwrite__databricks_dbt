{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Simulate Late-Arriving Data\n\nThis notebook simulates late-arriving orders by adding new data to source tables for specific dates.\n\n## Purpose:\nDemonstrates the value of insert_overwrite by showing how to efficiently refresh only affected dates.\n\n## Flow:\n1. Run dbt initially (creates mart tables with all data)\n2. Run this notebook (adds late orders to randomly selected dates in source)\n3. Run dbt with --full-refresh (insert_overwrite refreshes only those dates in mart)\n\n## Parameters:\n- `catalog`: Target Unity Catalog (default: main)\n- `schema`: Schema name (default: your_schema)\n\n## Auto-calculation:\nThe notebook automatically detects the date range from source tables and randomly selects 3 dates.\nEach date gets a random number of late-arriving orders (between 20-100) to simulate realistic scenarios.\nThis ensures the demo works regardless of when you run it and creates truly random test scenarios."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get parameters\ndbutils.widgets.text(\"catalog\", \"main\", \"Catalog Name\")\ndbutils.widgets.text(\"schema\", \"your_schema\", \"Schema Name\")\n\ncatalog = dbutils.widgets.get(\"catalog\")\nschema = dbutils.widgets.get(\"schema\")\n\n# Auto-calculate target dates based on actual data range\nprint(\"üîÑ Auto-calculating target dates based on actual data range...\")\n\n# Get the actual date range from source table\ndate_range_result = spark.sql(f\"\"\"\n    SELECT \n        MIN(order_date) as min_date,\n        MAX(order_date) as max_date,\n        DATEDIFF(MAX(order_date), MIN(order_date)) as days_span\n    FROM {catalog}.{schema}.orders_partitioned\n\"\"\").collect()[0]\n\nmin_date = date_range_result['min_date']\nmax_date = date_range_result['max_date']\ndays_span = date_range_result['days_span']\n\nprint(f\"  Data range: {min_date} to {max_date} ({days_span} days)\")\n\n# Randomly select 3 dates within the range, excluding today and recent days\n# Strategy: Pick 3 random offsets and ensure they're unique\n# Exclude the last 2 days to avoid today and very recent dates with potential ongoing transactions\nfrom datetime import timedelta, date\nimport random\n\n# Calculate how many days to exclude from the end\ntoday = date.today()\ndays_to_exclude = max(0, (max_date - today).days + 2)  # Exclude last 2 days including today\navailable_days = max(1, days_span - days_to_exclude + 1)  # Ensure at least 1 day available\n\n# Generate 3 unique random day offsets from the available range\nnum_dates_to_select = min(3, available_days)  # Can't select more than available\nrandom_offsets = random.sample(range(available_days), num_dates_to_select)\nrandom_offsets.sort()  # Sort for cleaner output\n\n# Calculate the actual dates\nselected_dates = [min_date + timedelta(days=offset) for offset in random_offsets]\ntarget_dates = [str(date) for date in selected_dates]\n\nprint(f\"  ‚úÖ Randomly selected {num_dates_to_select} dates: {', '.join(target_dates)}\")\nprint(f\"     (offsets: {', '.join(str(o) for o in random_offsets)} days from start)\")\nprint(f\"     (excluded last {days_to_exclude} days to avoid today and recent dates)\")\n\n# For each date, we'll append a random number of new orders (20-100)\n# This simulates late-arriving data being added to existing partitions\n# The approach: append mode adds NEW orders on top of existing data\norders_to_add_per_date = {\n    date: random.randint(20, 100) for date in target_dates\n}\n\nprint(f\"\\nCatalog: {catalog}\")\nprint(f\"Schema: {schema}\")\nprint(f\"Target Dates: {target_dates}\")\nprint(f\"New orders to add per date (late arrivals):\")\nfor date, count in orders_to_add_per_date.items():\n    print(f\"  {date}: +{count} orders (will be appended)\")\ntotal_orders = sum(orders_to_add_per_date.values())\nprint(f\"Total new orders to add: {total_orders}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get current counts for target dates BEFORE adding data\nfrom pyspark.sql import functions as F\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"BEFORE: Current counts for target dates\")\nprint(\"=\"*80)\n\nfor date in target_dates:\n    # Source table counts\n    count_partitioned_src = spark.sql(f\"\"\"\n        SELECT COUNT(*) as cnt\n        FROM {catalog}.{schema}.orders_partitioned\n        WHERE order_date = '{date}'\n    \"\"\").collect()[0]['cnt']\n\n    count_liquid_src = spark.sql(f\"\"\"\n        SELECT COUNT(*) as cnt\n        FROM {catalog}.{schema}.orders_liquid\n        WHERE order_date = '{date}'\n    \"\"\").collect()[0]['cnt']\n\n    # Mart table aggregations\n    mart_partitioned = spark.sql(f\"\"\"\n        SELECT total_orders\n        FROM {catalog}.{schema}.orders_mart_partitioned\n        WHERE order_date = '{date}'\n    \"\"\").collect()\n\n    mart_liquid_count = spark.sql(f\"\"\"\n        SELECT COUNT(*) as customer_count, SUM(total_orders) as total_orders\n        FROM {catalog}.{schema}.orders_mart_liquid\n        WHERE order_date = '{date}'\n    \"\"\").collect()\n\n    mart_part_orders = mart_partitioned[0]['total_orders'] if mart_partitioned else 0\n    mart_liq_orders = mart_liquid_count[0]['total_orders'] if mart_liquid_count and mart_liquid_count[0]['total_orders'] else 0\n\n    new_orders_to_add = orders_to_add_per_date[date]\n    expected_new_total = count_partitioned_src + new_orders_to_add\n\n    print(f\"\\n{date}:\")\n    print(f\"  Source Tables (CURRENT):\")\n    print(f\"    orders_partitioned: {count_partitioned_src:,} orders\")\n    print(f\"    orders_liquid:      {count_liquid_src:,} orders\")\n    print(f\"  Mart Tables (aggregated):\")\n    print(f\"    orders_mart_partitioned: {mart_part_orders:,} total_orders\")\n    print(f\"    orders_mart_liquid:      {mart_liq_orders:,} total_orders\")\n    print(f\"  ‚úÖ In Sync: Source and Mart both show {count_partitioned_src:,} orders\")\n    print(f\"  üìù Will append {new_orders_to_add} new orders (new total: {expected_new_total:,})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate late-arriving orders for each target date\nprint(f\"\\nGenerating late-arriving orders (appending to existing data)...\")\n\nfor date in target_dates:\n    # Get the random count for this specific date\n    num_orders = orders_to_add_per_date[date]\n    \n    # Create DataFrame with late orders for this specific date\n    df = spark.range(num_orders).select(\n        F.concat(\n            F.lit('ORD-LATE-'),\n            F.lit(date),\n            F.lit('-'),\n            F.lpad(F.col('id').cast('string'), 5, '0')\n        ).alias('order_id'),\n        \n        F.concat(\n            F.lit('CUST-'),\n            F.lpad(\n                F.when(F.rand() < 0.3, (F.rand() * 50 + 1).cast('int'))\n                 .when(F.rand() < 0.6, (F.rand() * 150 + 51).cast('int'))\n                 .otherwise((F.rand() * 300 + 201).cast('int')).cast('string'),\n                5, '0'\n            )\n        ).alias('customer_id'),\n        \n        F.lit(date).cast('date').alias('order_date'),\n        \n        F.concat(\n            F.lit('PROD-'),\n            F.lpad(\n                F.when(F.rand() < 0.4, (F.rand() * 20 + 1).cast('int'))\n                 .when(F.rand() < 0.7, (F.rand() * 30 + 21).cast('int'))\n                 .otherwise((F.rand() * 50 + 51).cast('int')).cast('string'),\n                4, '0'\n            )\n        ).alias('product_id'),\n        \n        F.when(F.rand() < 0.5, F.lit(1))\n         .when(F.rand() < 0.8, (F.rand() * 2 + 2).cast('int'))\n         .otherwise((F.rand() * 7 + 4).cast('int')).alias('quantity'),\n        \n        F.when(F.rand() < 0.4, F.round(9.99 + F.rand() * 40, 2))\n         .when(F.rand() < 0.7, F.round(50 + F.rand() * 100, 2))\n         .otherwise(F.round(150 + F.rand() * 350, 2)).alias('unit_price'),\n        \n        F.lit(None).cast('decimal(10,2)').alias('total_amount'),\n        \n        F.when(F.rand() < 0.60, F.lit('delivered'))\n         .when(F.rand() < 0.75, F.lit('shipped'))\n         .when(F.rand() < 0.87, F.lit('confirmed'))\n         .when(F.rand() < 0.95, F.lit('pending'))\n         .otherwise(F.lit('cancelled')).alias('status'),\n        \n        F.current_timestamp().alias('created_at')\n    )\n    \n    # Append to both source tables (adds NEW orders to existing data)\n    df.write.mode('append').insertInto(f\"{catalog}.{schema}.orders_partitioned\")\n    df.write.mode('append').insertInto(f\"{catalog}.{schema}.orders_liquid\")\n    \n    print(f\"  ‚úì Appended {num_orders} new orders for {date}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get counts AFTER adding data\nprint(\"\\n\" + \"=\"*80)\nprint(\"AFTER: Updated counts for target dates\")\nprint(\"=\"*80)\n\nfor date in target_dates:\n    # Source table counts\n    count_partitioned_src = spark.sql(f\"\"\"\n        SELECT COUNT(*) as cnt\n        FROM {catalog}.{schema}.orders_partitioned\n        WHERE order_date = '{date}'\n    \"\"\").collect()[0]['cnt']\n\n    count_liquid_src = spark.sql(f\"\"\"\n        SELECT COUNT(*) as cnt\n        FROM {catalog}.{schema}.orders_liquid\n        WHERE order_date = '{date}'\n    \"\"\").collect()[0]['cnt']\n\n    # Mart table aggregations (should still show OLD counts)\n    mart_partitioned = spark.sql(f\"\"\"\n        SELECT total_orders\n        FROM {catalog}.{schema}.orders_mart_partitioned\n        WHERE order_date = '{date}'\n    \"\"\").collect()\n\n    mart_liquid_count = spark.sql(f\"\"\"\n        SELECT COUNT(*) as customer_count, SUM(total_orders) as total_orders\n        FROM {catalog}.{schema}.orders_mart_liquid\n        WHERE order_date = '{date}'\n    \"\"\").collect()\n\n    mart_part_orders = mart_partitioned[0]['total_orders'] if mart_partitioned else 0\n    mart_liq_orders = mart_liquid_count[0]['total_orders'] if mart_liquid_count and mart_liquid_count[0]['total_orders'] else 0\n\n    difference = count_partitioned_src - mart_part_orders\n    added_count = orders_to_add_per_date[date]\n\n    print(f\"\\n{date}:\")\n    print(f\"  Source Tables (AFTER APPEND):\")\n    print(f\"    orders_partitioned: {count_partitioned_src:,} orders (+{added_count})\")\n    print(f\"    orders_liquid:      {count_liquid_src:,} orders (+{added_count})\")\n    print(f\"  Mart Tables (STILL OLD):\")\n    print(f\"    orders_mart_partitioned: {mart_part_orders:,} total_orders\")\n    print(f\"    orders_mart_liquid:      {mart_liq_orders:,} total_orders\")\n    print(f\"  ‚ö†Ô∏è  OUT OF SYNC: Source has {difference:,} more orders than Mart!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ LATE-ARRIVING DATA SIMULATION COMPLETE\")\nprint(\"=\"*80)\n\ntotal_new_orders = sum(orders_to_add_per_date.values())\nprint(f\"\\nAppended {total_new_orders} new orders to source tables\")\nprint(f\"Target dates: {', '.join(target_dates)}\")\nprint(f\"Orders appended per date (random delta):\")\nfor date in target_dates:\n    print(f\"  {date}: +{orders_to_add_per_date[date]} orders\")\n\nprint(f\"\\n‚ö†Ô∏è  CRITICAL: Source and Mart tables are now OUT OF SYNC!\")\nprint(f\"\\n  Source Tables:\")\nprint(f\"    ‚úÖ Have the NEW orders (appended on top of existing)\")\nfor date in target_dates:\n    print(f\"       {date}: +{orders_to_add_per_date[date]} orders\")\n\nprint(f\"\\n  Mart Tables (orders_mart_partitioned, orders_mart_liquid):\")\nprint(f\"    ‚ùå Still show OLD aggregations\")\nprint(f\"    ‚ùå Missing the appended orders in their totals\")\nprint(f\"    ‚ùå Revenue, customer counts, and other metrics are STALE\")\nprint(f\"\\nüí° This demonstrates a real-world scenario:\")\nprint(f\"   Late-arriving data from upstream systems (delayed feeds, corrections, etc.)\")\nprint(f\"   New orders are APPENDED to existing source data, creating sync gap with marts\")\nprint(f\"\\n‚ú® This notebook is part of the complete dbt_workflow job\")\nprint(f\"   The workflow automatically continues to:\")\nprint(f\"   - Run dbt build --full-refresh (fast! bypasses information_schema queries)\")\nprint(f\"   - insert_overwrite auto-detects which partitions to overwrite\")\nprint(f\"   - Only {len(target_dates)} affected partitions are refreshed\")\nprint(f\"   - Verify synchronization\")\nprint(f\"\\nüîë KEY INSIGHT:\")\nprint(f\"  --full-refresh with insert_overwrite does NOT replace all data!\")\nprint(f\"  It just skips incremental checks, while insert_overwrite remains data-driven\")\nprint(f\"  and only overwrites partitions with data in the result.\")\nprint(f\"\\nüìö REFERENCES:\")\nprint(f\"  - dbt insert_overwrite: https://docs.getdbt.com/docs/build/incremental-strategy#insert_overwrite\")\nprint(f\"  - Databricks Dynamic Partition Overwrites:\")\nprint(f\"    https://docs.databricks.com/aws/en/delta/selective-overwrite#dynamic-partition-overwrites-with-replace-using\")\nprint(f\"\\nüìä Efficiency gain:\")\nprint(f\"  - WITHOUT insert_overwrite: Replace entire table (~40 partitions)\")\nprint(f\"  - WITH dynamic overwrite:   Replace only {len(target_dates)} partitions (automatic!)\")\nprint(f\"  - Savings: {100 - (len(target_dates) / 40 * 100):.1f}% of partitions unchanged!\")\nprint(\"=\"*80)"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
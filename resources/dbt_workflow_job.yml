# Consolidated dbt workflow job
# This job orchestrates the complete dbt insert_overwrite demo workflow:
#   1. Setup and generate data (02a)
#   2. Initial dbt build with --full-refresh (creates all partitions)
#   3. Simulate late-arriving data (02b)
#   4. dbt build with --full-refresh (insert_overwrite STILL only updates affected partitions!)
#      Note: --full-refresh bypasses slow information_schema queries, but insert_overwrite
#            strategy is data-driven and only overwrites partitions that have data in the result
#   5. Verify synchronization (02c)

resources:
  jobs:
    dbt_workflow:
      name: "insert_overwrite__databricks_dbt_workflow_${bundle.environment}"

      # Job parameters
      parameters:
        - name: num_orders
          default: "20000"

      # Define environments for different task types
      environments:
        - environment_key: serverless_env
          spec:
            client: "4"
        - environment_key: dbt_sql_env
          spec:
            client: "4"
            dependencies:
              - "dbt-databricks>=1.11.0,<2.0.0"

      tasks:
        # Task 1: Setup and generate initial data (02a)
        - task_key: setup_and_generate
          notebook_task:
            notebook_path: "${workspace.file_path}/notebooks/02a_setup_and_generate_data"
            source: WORKSPACE
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              num_orders: "{{job.parameters.num_orders}}"
          environment_key: serverless_env
          timeout_seconds: 300

        # Task 2: Initial dbt build (creates mart tables)
        # Note: Uses --full-refresh to load all historical data on first run
        # The lookback_hours filter would miss historical data since created_at is in the past
        - task_key: dbt_initial_build
          depends_on:
            - task_key: setup_and_generate
          environment_key: dbt_sql_env
          dbt_task:
            project_directory: "${workspace.file_path}/dbt_project"
            commands:
              - "dbt build --full-refresh"
            warehouse_id: "${var.warehouse_id}"
            catalog: "${var.catalog}"
            schema: "${var.schema}"
          timeout_seconds: 900

        # Task 3: Simulate late-arriving data (02b)
        # Note: Notebook randomly selects 3 dates from actual data range
        # Random order counts (20-100) appended per date to simulate realistic scenarios
        - task_key: simulate_late_arrivals
          depends_on:
            - task_key: dbt_initial_build
          notebook_task:
            notebook_path: "${workspace.file_path}/notebooks/02b_simulate_late_arrivals"
            source: WORKSPACE
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
          environment_key: serverless_env
          timeout_seconds: 300

        # Task 4: dbt build to refresh affected partitions
        # Uses --full-refresh to bypass slow information_schema queries
        # insert_overwrite STILL only updates partitions with changed data (data-driven!)
        # The --full-refresh flag just tells dbt to skip incremental checks, not to replace all data
        # Runs models and tests to ensure data quality after refresh
        - task_key: dbt_refresh_partitions
          depends_on:
            - task_key: simulate_late_arrivals
          environment_key: dbt_sql_env
          dbt_task:
            project_directory: "${workspace.file_path}/dbt_project"
            commands:
              - "dbt build --full-refresh"
            warehouse_id: "${var.warehouse_id}"
            catalog: "${var.catalog}"
            schema: "${var.schema}"
          timeout_seconds: 900

        # Task 5: Verify synchronization (02c)
        # Note: Notebook auto-detects recently updated dates (no parameters needed)
        - task_key: verify_sync
          depends_on:
            - task_key: dbt_refresh_partitions
          notebook_task:
            notebook_path: "${workspace.file_path}/notebooks/02c_verify_sync"
            source: WORKSPACE
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
          environment_key: serverless_env
          timeout_seconds: 300

      # Timeout and retry settings
      timeout_seconds: 2700
      max_concurrent_runs: 1

      tags:
        project: "insert_overwrite__databricks_dbt"
        environment: "${bundle.environment}"
        job_type: "workflow"

      # Permissions
      permissions:
        - group_name: "users"
          level: "CAN_MANAGE_RUN"
